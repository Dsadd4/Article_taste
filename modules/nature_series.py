
import os
import sys
import time
import json
import schedule
import requests
from bs4 import BeautifulSoup
from PyQt5.QtWidgets import (QApplication, QMainWindow, QLabel, QVBoxLayout, QPushButton, QWidget, QScrollArea)
from pystray import Icon, MenuItem, Menu
from PIL import Image
import win32com.client
import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import requests
from bs4 import BeautifulSoup

# æ¯æ‰¹æŠ“å– 10 é¡µï¼Œæ¯æ¬¡å¯åŠ¨ 10 ä¸ªçº¿ç¨‹
BATCH_SIZE = 5
MAX_THREADS = 5
MAX_PAGES = 100  # æœ€å¤šæŠ“100é¡µï¼Œé˜²æ­¢æ— é™çˆ¬å–

# conda activate scrath_new

# === CONFIGURATION ===
CONFIG_FILE = 'config.json'
CACHE_FILE = 'cache.json'

# Load configuration
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, 'r') as f:
        CONFIG = json.load(f)
else:
    CONFIG = {"keywords": []}


#=== å¤„ç†natureç³»åˆ—æœŸåˆŠ ===
# æ˜ å°„ journal_name åˆ°æœŸåˆŠç¼©å†™
JOURNAL_MAPPING = {
    'nature_biotechnology': 'nbt',
    'nature_methods': 'nmeth',
    'nature_machine_intelligence': 'natmachintell',
    'nature': 'nature',
    'nature_computer_science': 'natcomputsci',
    'nature_communications': 'ncomms'
}




def sanitize_filename(filename):
    # å»é™¤ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
    return re.sub(r'[\/:*?"<>|]', '', filename)

def create_download_dir():
    today = datetime.today().strftime('%Y-%m-%d')
    download_path = os.path.join(os.getcwd(), 'download', today)
    if not os.path.exists(download_path):
        os.makedirs(download_path, exist_ok=True)
    return download_path

def download_pdf(title, link):
    try:
        download_path = create_download_dir()
        pdf_link = link.replace('/abs/', '/pdf/')

        #debug
        print(f"we now try to download{pdf_link } ")

        response = requests.get(pdf_link)
        if response.status_code == 200:
            sanitized_title = sanitize_filename(title)
            filename = os.path.join(download_path, f"{sanitized_title}.pdf")
            with open(filename, 'wb') as f:
                f.write(response.content)
            print(f"âœ… Downloaded: {filename}")
        else:
            print(f"âŒ Failed to download: {pdf_link} (Status Code: {response.status_code})")
    except Exception as e:
        print(f"âŒ Error downloading {link}: {str(e)}")


import concurrent.futures
import requests
from bs4 import BeautifulSoup


# === CACHE HANDLING ===
def save_cache(source, data):
    cache = load_all_cache()
    cache[source] = {"data": data, "timestamp": time.strftime('%Y-%m-%d')}
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f)

def load_cache(source):
    cache = load_all_cache()
    return cache.get(source, {}).get("data", [])

def load_all_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    return {}

def is_cached(source):
    cache = load_all_cache()
    cached_date = cache.get(source, {}).get("timestamp", "")
    return cached_date == time.strftime('%Y-%m-%d')


def fetch_page(journal_name, page_number):
    try:
        journal_abbr = JOURNAL_MAPPING[journal_name] 
        url = f"https://www.nature.com/{journal_abbr}/articles?searchType=journalSearch&sort=PubDate&year=2025&page={page_number}"
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='app-article-list-row__item')
        if not articles:
            return []

        papers = []
        for article in articles:
            link_tag = article.find('a', class_='c-card__link u-link-inherit')
            if link_tag and 'href' in link_tag.attrs:
                link = f"https://www.nature.com{link_tag['href']}"
            else:
                continue

            title = link_tag.get_text(strip=True)

            # æ‘˜è¦
            summary = article.find('div', class_='c-card__summary')
            if summary:
                p_tag = summary.find('p')
                summary_text = p_tag.get_text(strip=True) if p_tag else "No summary available"
            else:
                summary_text = "No summary available"

            # å‘å¸ƒæ—¥æœŸ
            date_published = article.find('time', class_="c-meta__item c-meta__item--block-at-lg")
            date_published = date_published.get_text(strip=True) if date_published else "No date available"

            papers.append((title, link, summary_text, date_published))

        return papers

    except Exception as e:
        print(f"Error fetching page {page_number}: {e}")
        return []


def fetch_nature_series(journal_name):
    try:
        if is_cached(journal_name):
            return load_cache(journal_name)

        papers = []
        page_number = 1

        while True:
            print(f"ğŸš€ å¼€å§‹æŠ“å–ç¬¬ {page_number} é¡µåˆ°ç¬¬ {page_number + BATCH_SIZE - 1} é¡µ")

            batch_results = []
            stop_flag = False

            # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶è¡ŒæŠ“å–
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
                future_to_page = {
                    executor.submit(fetch_page, journal_name, page): page
                    for page in range(page_number, page_number + BATCH_SIZE)
                }
                for future in concurrent.futures.as_completed(future_to_page):
                    page = future_to_page[future]
                    try:
                        result = future.result()
                        if not result:
                            print(f"âš ï¸ ç¬¬ {page} é¡µä¸ºç©ºï¼Œå¯èƒ½æ˜¯æœ«é¡µï¼Œå‡†å¤‡åœæ­¢æŠ“å–ã€‚")
                            stop_flag = True
                        else:
                            batch_results.extend(result)
                    except Exception as e:
                        print(f"âš ï¸ æŠ“å–ç¬¬ {page} é¡µå¤±è´¥: {e}")

            # åˆå¹¶å½“å‰æ‰¹æ¬¡ç»“æœ
            papers.extend(batch_results)

            if stop_flag or page_number + BATCH_SIZE > MAX_PAGES:
                print("âœ… æŠ“å–å®Œæˆï¼")
                break

            # æ›´æ–°ä¸‹ä¸€æ‰¹æŠ“å–çš„èµ·ç‚¹
            page_number += BATCH_SIZE

        # ä¿å­˜ç»“æœï¼ˆç¼“å­˜ï¼‰
        save_cache(journal_name, papers)
        return papers

    except Exception as e:
        print(f"âŒ Error fetching {journal_name}: {e}")
        return [(f"Error fetching {journal_name}: {str(e)}", "", "", "")]
