 




import os
import sys
import time
import json
import schedule
import requests
from bs4 import BeautifulSoup
from PyQt5.QtWidgets import (QApplication, QMainWindow, QLabel, QVBoxLayout, QPushButton, QWidget, QScrollArea)
from pystray import Icon, MenuItem, Menu
from PIL import Image
import win32com.client
import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re




response = requests.get("https://www.cell.com/cell/issue?pii=S0092-8674(24)X0002-1")

print(response.text)


from selenium import webdriver

options = webdriver.ChromeOptions()
options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
driver = webdriver.Chrome(options=options)

url = 'https://www.cell.com/cell/issue?pii=S0092-8674(24)X0002-1'
driver.get(url)
print(driver.page_source)  # è·å–é¡µé¢å†…å®¹
driver.quit()

assert(0)














# === CONFIGURATION ===
CONFIG_FILE = 'config.json'
CACHE_FILE = 'cache.json'

# === CACHE HANDLING ===
def save_cache(source, data):
    cache = load_all_cache()
    cache[source] = {"data": data, "timestamp": time.strftime('%Y-%m-%d')}
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f)

def load_cache(source):
    cache = load_all_cache()
    return cache.get(source, {}).get("data", [])

def load_all_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    return {}

def is_cached(source):
    cache = load_all_cache()
    cached_date = cache.get(source, {}).get("timestamp", "")
    return cached_date == time.strftime('%Y-%m-%d')











import math
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordclouds(set_column=4):
    cache = load_all_cache()
    if not cache:
        print("No data in cache.")
        return
    
    row = math.ceil(len(cache) / set_column)  # å‘ä¸Šå–æ•´ï¼Œé˜²æ­¢é—æ¼
    figures, ax = plt.subplots(row, set_column, figsize=(set_column * 5, row * 3))

    # æ ‡å‡†åŒ– ax ä¸ºäºŒç»´æ•°ç»„ï¼Œé˜²æ­¢å‡ºç°ä¸€ç»´æˆ–æ ‡é‡æƒ…å†µ
    if row == 1:
        ax = [ax]
    if set_column == 1:
        ax = [[a] for a in ax]

    row_number = 0
    column_number = 0

    for item in cache.keys():
        data = load_cache(item)
        if not data:
            continue
        
        text, *_ = zip(*data)  # åªå–ç¬¬ä¸€åˆ—
        titles = list(text)
        text = " ".join(titles)
        
        # ç”Ÿæˆè¯äº‘
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        # ç»˜åˆ¶è¯äº‘
        ax[row_number][column_number].imshow(wordcloud, interpolation='bilinear')
        ax[row_number][column_number].axis('off')  # å…³é—­åæ ‡è½´
        ax[row_number][column_number].set_title(item)  # æ·»åŠ æ ‡é¢˜
        
        column_number += 1
        if column_number == set_column:  # åˆ—æ»¡æ—¶æ¢è¡Œ
            column_number = 0
            row_number += 1
    
    # åˆ é™¤å¤šä½™çš„ç©ºç™½å­å›¾
    for i in range(row_number * set_column + column_number, row * set_column):
        figures.delaxes(ax[i // set_column][i % set_column])

    plt.tight_layout()
    plt.show()

# åŠ è½½ç¼“å­˜


# è°ƒç”¨å‡½æ•°
generate_wordclouds()






assert(0)


import requests

import requests
from datetime import datetime, timedelta
import requests
from datetime import datetime, timedelta
import concurrent.futures

import requests
from datetime import datetime, timedelta
import concurrent.futures

import requests
from datetime import datetime, timedelta

import requests
from datetime import datetime, timedelta

def fetch_biorxiv_medrxiv():
    try:
        if is_cached('biorxiv_medrxiv'):
            return load_cache('biorxiv_medrxiv')

        # Get today's date
        today = datetime.today()

        # Calculate the date for one week ago from today
        one_week_ago = today - timedelta(weeks=1)

        # Format the dates to the required string format for the API (YYYY-MM-DD)
        start_date = one_week_ago.strftime('%Y-%m-%d')
        end_date = today.strftime('%Y-%m-%d')

        papers = []
        offset = 0  # Start from the first batch
        while True:
            # Create the URL with the dynamic date range and the offset
            url = f"https://api.biorxiv.org/details/biorxiv/{start_date}/{end_date}/{offset}"
            response = requests.get(url)
            data = response.json()

            # Check if there are any papers in the response
            collection = data.get('collection', [])
            if not collection:
                break  # Exit the loop if no more papers are found

            # Process the papers in the current batch
            for item in collection:
                # Extract the DOI
                doi = item.get('doi', 'N/A')
                link = f"https://www.biorxiv.org/content/{doi}"

                # Extract the title
                title = item.get('title', 'N/A')

                # Extract the publication date
                date = item.get('date', 'N/A')

                # Create the paper tuple and add it to the list
                papers.append((title, link, date, []))

            # Increment the offset for the next batch
            offset += 100
            print(offset)
        # Save the result to cache
        save_cache('biorxiv_medrxiv', papers)
        return papers

    except Exception as e:
        return [(f"Error fetching bioRxiv/medRxiv: {str(e)}", "")]








import requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_biorxiv_medrxiv():
    try:
        if is_cached('biorxiv_medrxiv'):
            return load_cache('biorxiv_medrxiv')

        # Get today's date
        today = datetime.today()

        # Calculate the date for one week ago from today
        one_week_ago = today - timedelta(weeks=1)

        # Format the dates to the required string format for the API (YYYY-MM-DD)
        start_date = one_week_ago.strftime('%Y-%m-%d')
        end_date = today.strftime('%Y-%m-%d')

        papers = []
        max_threads = 3  # Number of threads to use
        offset = 0  # Start from the first batch

        def fetch_batch(offset):
            # Create the URL with the dynamic date range and the offset
            url = f"https://api.biorxiv.org/details/biorxiv/{start_date}/{end_date}/{offset}"
            response = requests.get(url)
            data = response.json()

            # Process the papers in the current batch
            batch_papers = []
            collection = data.get('collection', [])
            for item in collection:
                # Extract the DOI
                doi = item.get('doi', 'N/A')
                link = f"https://www.biorxiv.org/content/{doi}"

                # Extract the title
                title = item.get('title', 'N/A')

                # Extract the publication date
                date = item.get('date', 'N/A')

                # Create the paper tuple and add it to the list
                batch_papers.append((title, link, date, []))
            return batch_papers

        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = []
            while True:
                # Dispatch multiple requests
                for i in range(max_threads):
                    futures.append(executor.submit(fetch_batch, offset))
                    offset += 100  # Increment offset for the next batch
                print(offset)
                # Wait for all threads to finish
                for future in as_completed(futures):
                    papers.extend(future.result())

                # If no more papers are fetched, break the loop
                if len(futures) < max_threads or len(futures[-1].result()) == 0:
                    break

        # Save the result to cache
        save_cache('biorxiv_medrxiv', papers)
        return papers

    except Exception as e:
        return [(f"Error fetching bioRxiv/medRxiv: {str(e)}", "")]



print(fetch_biorxiv_medrxiv())
assert(0)


from paperscraper.get_dumps import biorxiv, medrxiv, chemrxiv

biorxiv(start_date="2025-03-01", end_date="2025-03-02")
assert(0)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# è®¾ç½® WebDriver è·¯å¾„
driver = webdriver.Chrome()

# æ‰“å¼€ç›®æ ‡ç½‘é¡µ
driver.get("https://www.biorxiv.org/content/early/recent?page=3")

# ç­‰å¾…é¡µé¢åŠ è½½
driver.implicitly_wait(10)

# æ‰¾åˆ°å¹¶æ¨¡æ‹Ÿç‚¹å‡»
button = driver.find_element(By.ID, "button-id")  # æ ¹æ®é¡µé¢å…ƒç´ å®šä½
action = ActionChains(driver)
action.click(button).perform()

# ç­‰å¾…å¹¶æŠ“å–ç½‘é¡µå†…å®¹
print(driver.page_source)

# å…³é—­æµè§ˆå™¨
driver.quit()



assert(0)






from playwright.sync_api import sync_playwright

def scrape():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        page.goto("https://www.biorxiv.org/content/early/recent?page=3", timeout=60000)

        # ç­‰å¾…é¡µé¢åŠ è½½å¹¶ç‚¹å‡»æŒ‰é’®
        try:
            page.wait_for_selector("text='Verify'", timeout=20000)
            page.click("text='Verify'")
            print("ç‚¹å‡»æŒ‰é’®æˆåŠŸ")
        except Exception as e:
            print(f"æœªæ‰¾åˆ°æŒ‰é’®æˆ–ç‚¹å‡»å¤±è´¥: {e}")

        # è·å–HTMLå†…å®¹
        html = page.content()
        print(html)

        browser.close()

from playwright.sync_api import sync_playwright
from playwright_stealth import stealth
import time

from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync  # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥
import time

def scrape():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        # ä½¿ç”¨ stealth_sync æ¥åº”ç”¨åæ£€æµ‹åŠŸèƒ½
        stealth_sync(page)

        # è®¿é—®ç›®æ ‡é¡µé¢
        page.goto("https://www.biorxiv.org/content/early/recent?page=3", timeout=60000)

        try:
            # ç­‰å¾…å¹¶ç‚¹å‡»æ ‡ç­¾
            label = page.locator('label.cb-lb')
            label.wait_for(state="visible", timeout=10000)
            label.click()

            print("ç‚¹å‡»æ ‡ç­¾æˆåŠŸ")

            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            page.wait_for_load_state('networkidle')

        except Exception as e:
            print(f"æœªæ‰¾åˆ°æ ‡ç­¾æˆ–ç‚¹å‡»å¤±è´¥: {e}")

        # è·å–HTMLå†…å®¹
        html = page.content()
        print(html)

        browser.close()

scrape()






assert(0)








import requests
from bs4 import BeautifulSoup
import concurrent.futures
import time

# æ¯æ‰¹æŠ“å– 10 é¡µï¼Œæ¯æ¬¡å¯åŠ¨ 10 ä¸ªçº¿ç¨‹
BATCH_SIZE = 10
MAX_THREADS = 10
MAX_PAGES = 100  # æœ€å¤šæŠ“å– 100 é¡µ


from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# é…ç½®æµè§ˆå™¨
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ä¸æ˜¾ç¤ºçª—å£
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# å¯åŠ¨æµè§ˆå™¨
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# æ‰“å¼€ç›®æ ‡ç½‘é¡µ
url = "https://www.biorxiv.org/content/early/recent?page=1"
driver.get(url)

# è·å–åŠ è½½åçš„ç½‘é¡µå†…å®¹
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

# æ‰“å°å†…å®¹
print(soup.prettify())

# å…³é—­æµè§ˆå™¨
driver.quit()



with open("example.html",'w',encoding='utf-8') as f:
    f.write(str(soup))

def fetch_page_biorxiv(page_number):
    try:
        url = f"https://www.biorxiv.org/content/early/recent?page={page_number}"
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            print(f"âš ï¸ ç¬¬ {page_number} é¡µè·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        with open("example.html",'w',encoding='utf-8') as f:
            f.write(str(soup))
        
        articles = soup.find_all('div', class_='highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix')
        if not articles:
            print(f"âš ï¸ ç¬¬ {page_number} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œå¯èƒ½æ˜¯æœ«é¡µã€‚")
            return []

        papers = []
        for article in articles:
            # æ ‡é¢˜å’Œé“¾æ¥
            link_tag = article.find('a', class_='highwire-cite-linked-title')
            if link_tag and 'href' in link_tag.attrs:
                link = f"https://www.biorxiv.org{link_tag['href']}"
                title = link_tag.find('span', class_='highwire-cite-title').get_text(strip=True)
            else:
                continue

            # æ‘˜è¦
            summary_tag = article.find('div', class_='highwire-cite-snippet')
            summary = summary_tag.get_text(strip=True) if summary_tag else "No summary available"

            # å‘å¸ƒæ—¥æœŸ
            date_tag = article.find('span', class_='highwire-cite-metadata-date')
            date_published = date_tag.get_text(strip=True) if date_tag else "No date available"

            papers.append((title, link, summary, date_published))

        return papers

    except Exception as e:
        print(f"âŒ æŠ“å–ç¬¬ {page_number} é¡µå‡ºé”™: {e}")
        return []


def fetch_biorxiv():
    try:
        if is_cached('biorxiv'):
            return load_cache('biorxiv')

        papers = []
        page_number = 1

        while True:
            print(f"ğŸš€ å¼€å§‹æŠ“å–ç¬¬ {page_number} é¡µåˆ°ç¬¬ {page_number + BATCH_SIZE - 1} é¡µ")

            batch_results = []
            stop_flag = False

            # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶è¡ŒæŠ“å–
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
                future_to_page = {
                    executor.submit(fetch_page_biorxiv, page): page
                    for page in range(page_number, page_number + BATCH_SIZE)
                }

                for future in concurrent.futures.as_completed(future_to_page):
                    page = future_to_page[future]
                    try:
                        result = future.result()
                        if not result:
                            print(f"âš ï¸ ç¬¬ {page} é¡µä¸ºç©ºï¼Œå¯èƒ½æ˜¯æœ«é¡µï¼Œå‡†å¤‡åœæ­¢æŠ“å–ã€‚")
                            stop_flag = True
                        else:
                            batch_results.extend(result)
                    except Exception as e:
                        print(f"âŒ æŠ“å–ç¬¬ {page} é¡µå¤±è´¥: {e}")

            # åˆå¹¶å½“å‰æ‰¹æ¬¡ç»“æœ
            papers.extend(batch_results)

            if stop_flag or page_number + BATCH_SIZE > MAX_PAGES:
                print("âœ… æŠ“å–å®Œæˆï¼")
                break

            # æ›´æ–°ä¸‹ä¸€æ‰¹æŠ“å–çš„èµ·ç‚¹
            page_number += BATCH_SIZE

        # ä¿å­˜ç»“æœï¼ˆç¼“å­˜ï¼‰
        save_cache('biorxiv', papers)
        return papers

    except Exception as e:
        print(f"âŒ Error fetching biorxiv: {e}")
        return [(f"Error fetching biorxiv: {str(e)}", "", "", "")]


# è°ƒç”¨å‡½æ•°
if __name__ == "__main__":
    start_time = time.time()
    papers = fetch_biorxiv()
    end_time = time.time()
    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…± {len(papers)} ç¯‡æ–‡ç« ï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")

    # æ‰“å°å‰ 5 æ¡è®°å½•
    for paper in papers[:5]:
        print(f"ğŸ“Œ æ ‡é¢˜: {paper[0]}\nğŸ”— é“¾æ¥: {paper[1]}\nğŸ“„ æ‘˜è¦: {paper[2]}\nğŸ“… æ—¥æœŸ: {paper[3]}\n{'-'*50}")




assert(0)


import requests
from bs4 import BeautifulSoup

def fetch_nature_biotechnology():
    try:
        if is_cached('nature_biotechnology'):
            return load_cache('nature_biotechnology')
        
        url = "https://www.nature.com/nbt/articles?year=2025"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        papers = []
        
        # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« çš„ li æ ‡ç­¾
        articles = soup.find_all('li', class_='app-article-list-row__item')

        for article in articles:
            # print(article)
            # æå–æ–‡ç« çš„é“¾æ¥
            # link = f"https://www.nature.com{article.find('a', class_='c-card__link u-link-inherit')['href']}"
            link_tag = article.find('a', class_='c-card__link u-link-inherit')
            if link_tag and 'href' in link_tag.attrs:
                link = f"https://www.nature.com{link_tag['href']}"
            else:
                # å¦‚æœé“¾æ¥æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œåˆ™è·³å‡ºå¾ªç¯
                print("Link format not as expected, skipping article.")
                break
            # æå–æ ‡é¢˜
            title = article.find('a', class_='c-card__link u-link-inherit').get_text(strip=True)
            
            # å¯é€‰ï¼šæå–æ‘˜è¦
            summary = article.find('div', class_='c-card__summary')  # å®šä½åˆ°åŒ…å«æ‘˜è¦çš„ div æ ‡ç­¾
            if summary:
                p_tag = summary.find('p')  # æ‰¾åˆ° div æ ‡ç­¾ä¸­çš„ p æ ‡ç­¾
                summary_text = p_tag.get_text(strip=True) if p_tag else "No summary available"
            else:
                summary_text = "No summary available"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_published = article.find('time', class_="c-meta__item c-meta__item--block-at-lg" ).get_text(strip=True)
            

            
            papers.append((title, link, summary_text, date_published))
            # assert()
        print(papers)
        assert()
        save_cache('nature_biotechnology', papers)
        return papers
    except Exception as e:
        return [(f"Error fetching Nature Biotechnology: {str(e)}", "", "", "")]


# url = "https://www.nature.com/nbt/articles?year=2025"
# response = requests.get(url)
# soup = BeautifulSoup(response.text, 'html.parser')

# papers = []
# # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« çš„ li æ ‡ç­¾
# articles = soup.find_all('li', class_='app-article-list-row__item')

# for article in articles:
#     # print(article)
#     # æå–æ–‡ç« çš„é“¾æ¥
#     link = f"https://www.nature.com{article.find('a', class_='c-card__link u-link-inherit')['href']}"
    
#     # æå–æ ‡é¢˜
#     title = article.find('a', class_='c-card__link u-link-inherit').get_text(strip=True)
    
#     # å¯é€‰ï¼šæå–æ‘˜è¦
#     summary = article.find('div', class_='c-card__summary')  # å®šä½åˆ°åŒ…å«æ‘˜è¦çš„ div æ ‡ç­¾
#     if summary:
#         p_tag = summary.find('p')  # æ‰¾åˆ° div æ ‡ç­¾ä¸­çš„ p æ ‡ç­¾
#         summary_text = p_tag.get_text(strip=True) if p_tag else "No summary available"
#     else:
#         summary_text = "No summary available"
    
#     # æå–å‘å¸ƒæ—¥æœŸ
#     date_published = article.find('time', class_="c-meta__item c-meta__item--block-at-lg" ).get_text(strip=True)
    
#     papers.append((title, link, summary_text, date_published))
#     # assert()
# print(papers)
# assert()



print(fetch_nature_biotechnology())
assert()






# === test part===
import requests
from bs4 import BeautifulSoup
def fetch_nature_biotech():
    try:
        if is_cached('nature_biotech'):
            return load_cache('nature_biotech')
        url = "https://www.nature.com/nbt/articles?year=2025"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        print(soup)
        papers = []
        articles = soup.find_all('article', class_='card')
        for article in articles:
            link = article.find('a', class_='card-title')['href']
            title = article.find('a', class_='card-title').get_text(strip=True)
            papers.append((title, f"https://www.nature.com{link}"))
        save_cache('nature_biotech', papers)
        return papers
    except Exception as e:
        return [(f"Error fetching Nature Biotechnology: {str(e)}", "")]

url = "https://www.nature.com/nbt/articles?year=2025"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
with open("output.html", "w", encoding="utf-8") as f:
    f.write(soup.prettify())

print(soup)
# articles = soup.find_all('article', class_='card')
# print(articles )


# print(fetch_nature_biotech())